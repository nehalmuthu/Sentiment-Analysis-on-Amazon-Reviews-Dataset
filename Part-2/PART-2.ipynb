{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 869
        },
        "id": "CEerL0ascuSI",
        "outputId": "ec192883-0be4-4750-9b90-bf176bb1ac25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==4.3.0\n",
            "  Downloading gensim-4.3.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting FuzzyTM>=0.4.0\n",
            "  Downloading FuzzyTM-2.0.5-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim==4.3.0) (6.3.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from gensim==4.3.0) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from gensim==4.3.0) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from FuzzyTM>=0.4.0->gensim==4.3.0) (1.3.5)\n",
            "Collecting pyfume\n",
            "  Downloading pyFUME-0.2.25-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->FuzzyTM>=0.4.0->gensim==4.3.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->FuzzyTM>=0.4.0->gensim==4.3.0) (2022.7.1)\n",
            "Collecting simpful\n",
            "  Downloading simpful-2.10.0-py3-none-any.whl (31 kB)\n",
            "Collecting fst-pso\n",
            "  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->FuzzyTM>=0.4.0->gensim==4.3.0) (1.15.0)\n",
            "Collecting miniful\n",
            "  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from simpful->pyfume->FuzzyTM>=0.4.0->gensim==4.3.0) (2.25.1)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim==4.3.0) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim==4.3.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim==4.3.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim==4.3.0) (1.26.14)\n",
            "Building wheels for collected packages: fst-pso, miniful\n",
            "  Building wheel for fst-pso (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20443 sha256=f3f2b0ada9c17e79cccd860f080dd147200fcc60249c2314bcf89efd984780da\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/65/c4/d27eeee9ba3fc150a0dae150519591103b9e0dbffde3ae77dc\n",
            "  Building wheel for miniful (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3530 sha256=8e6d3dcb487fbf1e288f68d2ede5c2ac91dafa7525aec46391a338a6051bde3d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/d9/a0/ddd93af16d5855dd9bad417623e70948fdac119d1d34fb17c8\n",
            "Successfully built fst-pso miniful\n",
            "Installing collected packages: simpful, miniful, fst-pso, pyfume, FuzzyTM, gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed FuzzyTM-2.0.5 fst-pso-1.8.1 gensim-4.3.0 miniful-0.0.6 pyfume-0.2.25 simpful-2.10.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gensim"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install gensim==4.3.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "YF8oYBwl8aZi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZfA7ofq-Jhw"
      },
      "source": [
        "# 1. DATASET GENERATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "987yYgTIoHWX"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('./data.tsv', sep='\\t', on_bad_lines='skip',low_memory=False)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYHXbId39cwR"
      },
      "outputs": [],
      "source": [
        "#Keep Reviews and Ratings\n",
        "reviews=data[[\"review_body\",\"star_rating\"]].copy()\n",
        "reviews = reviews[reviews[\"review_body\"].notna()]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfMCF2Jwoehu"
      },
      "source": [
        "### Relabelling Ratings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-mDeNYQ9grS"
      },
      "outputs": [],
      "source": [
        "\n",
        "# We form three classes and select 20000 reviews randomly from each class.\n",
        "reviews[\"star_rating\"]=reviews[\"star_rating\"].replace('1',1)\n",
        "reviews[\"star_rating\"]=reviews[\"star_rating\"].replace(2,1)\n",
        "reviews[\"star_rating\"]=reviews[\"star_rating\"].replace('2',1)\n",
        "reviews[\"star_rating\"]=reviews[\"star_rating\"].replace('3',2)\n",
        "reviews[\"star_rating\"]=reviews[\"star_rating\"].replace(4,3)\n",
        "reviews[\"star_rating\"]=reviews[\"star_rating\"].replace('4',3)\n",
        "reviews[\"star_rating\"]=reviews[\"star_rating\"].replace(5,3)\n",
        "reviews[\"star_rating\"]=reviews[\"star_rating\"].replace('5',3)\n",
        "\n",
        "\n",
        "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html\n",
        "class1_df = reviews[reviews[\"star_rating\"]==1]\n",
        "sample1=class1_df.sample(n = 20000,random_state=47)\n",
        "sample1 = sample1.reset_index(drop=True)\n",
        "class2_df=reviews[reviews[\"star_rating\"]==2]\n",
        "sample2=class2_df.sample(n = 20000,random_state=47)\n",
        "sample2 = sample2.reset_index(drop=True)\n",
        "class3_df = reviews[reviews[\"star_rating\"]==3]\n",
        "sample3=class3_df.sample(n = 20000,random_state=47)\n",
        "sample3 = sample3.reset_index(drop=True)\n",
        "\n",
        "reviews_df=pd.concat([sample1,sample2,sample3],axis=0,ignore_index=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "0YqidIwe9-4u",
        "outputId": "80a2c8f5-5532-4a61-b065-2ceacec92c55"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_body</th>\n",
              "      <th>star_rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rancid smell.. Threw it away, smelled like it ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This flavor is gross What a nasty flavor!! The...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I was not a fan of this product. It ... I was ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Not worth the investment I have been using the...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Wow I don't mean to be rude about it but wow! ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59995</th>\n",
              "      <td>Vi-Tae Shea Butter Soap This is my second purc...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59996</th>\n",
              "      <td>Four Stars Not working buy how they handled my...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59997</th>\n",
              "      <td>The smell is awesome and it leaves my hair so ...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59998</th>\n",
              "      <td>Very Pretty Hair Really loved this hair. I wou...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59999</th>\n",
              "      <td>Great natural product! In past experiences I h...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>60000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             review_body  star_rating\n",
              "0      rancid smell.. Threw it away, smelled like it ...            1\n",
              "1      This flavor is gross What a nasty flavor!! The...            1\n",
              "2      I was not a fan of this product. It ... I was ...            1\n",
              "3      Not worth the investment I have been using the...            1\n",
              "4      Wow I don't mean to be rude about it but wow! ...            1\n",
              "...                                                  ...          ...\n",
              "59995  Vi-Tae Shea Butter Soap This is my second purc...            3\n",
              "59996  Four Stars Not working buy how they handled my...            3\n",
              "59997  The smell is awesome and it leaves my hair so ...            3\n",
              "59998  Very Pretty Hair Really loved this hair. I wou...            3\n",
              "59999  Great natural product! In past experiences I h...            3\n",
              "\n",
              "[60000 rows x 2 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reviews_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNFcdVClDGmO"
      },
      "outputs": [],
      "source": [
        "reviews_df.to_csv('data.csv', header=True, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNMSH-KzpRGI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u6Xb_2i-M-2"
      },
      "source": [
        "# 2. WORD EMBEDDING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErgIZV3DpT_G"
      },
      "source": [
        "### Train - Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "e1MbQ_uwpRMQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review_data = pd.read_csv('/content/data.csv')\n"
      ],
      "metadata": {
        "id": "Qb5RzZ-YrGKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "QMrOouI2pSjK"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = train_test_split(review_data, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRMsxxS3o9qM"
      },
      "source": [
        "## a) Loading pretrained W2V Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QD0kL_ey-fSB"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "wv = api.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A2pHaTyqHd7"
      },
      "source": [
        "### Example 1 - King - Man + Woman = Queen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44ay3F2nBYAb",
        "outputId": "89c989ea-05eb-43b6-9012-b4b4f20f688a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 similar words to 'king-man+woman':  [('king', 0.8449392318725586), ('queen', 0.7300517559051514), ('monarch', 0.645466148853302), ('princess', 0.6156251430511475), ('crown_prince', 0.5818676352500916), ('prince', 0.5777117609977722), ('kings', 0.5613663792610168), ('sultan', 0.5376775860786438), ('Queen_Consort', 0.5344247817993164), ('queens', 0.5289887189865112)]\n"
          ]
        }
      ],
      "source": [
        "vec_king = wv['king']\n",
        "vec_man = wv['man']\n",
        "vec_woman = wv['woman']\n",
        "\n",
        "isQueen=vec_king-vec_man+vec_woman\n",
        "\n",
        "similar_words = wv.similar_by_vector(isQueen, topn=10)\n",
        "print(\"Top 10 similar words to 'king-man+woman': \", similar_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3qqFM4nykuI",
        "outputId": "daa03d09-1e2d-4c15-fdde-f60ac2ea04d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+--------------------+\n",
            "|      Word     |       Score        |\n",
            "+---------------+--------------------+\n",
            "|      king     | 0.8449392318725586 |\n",
            "|     queen     | 0.7300517559051514 |\n",
            "|    monarch    | 0.645466148853302  |\n",
            "|    princess   | 0.6156251430511475 |\n",
            "|  crown_prince | 0.5818676352500916 |\n",
            "|     prince    | 0.5777117609977722 |\n",
            "|     kings     | 0.5613663792610168 |\n",
            "|     sultan    | 0.5376775860786438 |\n",
            "| Queen_Consort | 0.5344247817993164 |\n",
            "|     queens    | 0.5289887189865112 |\n",
            "+---------------+--------------------+\n"
          ]
        }
      ],
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "# Define the table headers\n",
        "table = PrettyTable()\n",
        "table.field_names = [\"Word\", \"Score\"]\n",
        "\n",
        "# Add the data to the table\n",
        "for row in similar_words:\n",
        "    table.add_row(row)\n",
        "\n",
        "# Print the table\n",
        "print(table)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vCwySrtqJ9B"
      },
      "source": [
        "### Example 2 - Excellent ∼ Outstanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7mo_qSKCFHt",
        "outputId": "c22095f6-a117-4c35-c1b7-0beea6243c21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 similar words to 'Excellent':  [('excellent', 1.0), ('terrific', 0.7409726977348328), ('superb', 0.7062715888023376), ('exceptional', 0.681470513343811), ('fantastic', 0.6802847385406494), ('good', 0.6442928910255432), ('great', 0.6124600172042847), ('Excellent', 0.6091997623443604), ('impeccable', 0.5980967283248901), ('exemplary', 0.5959650278091431)]\n"
          ]
        }
      ],
      "source": [
        "vec_excellent=wv['excellent']\n",
        "similar_words = wv.similar_by_vector(vec_excellent, topn=10)\n",
        "print(\"Top 10 similar words to 'Excellent': \", similar_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYlqvlq32D5U",
        "outputId": "300e88e0-45a1-4f7c-e3e1-ccbeec52b778"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity between 'excellent' and 'outstanding':  0.55674857\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Calculate and print the semantic similarity of \"excellent\" and \"outstanding\"\n",
        "excellent_outstanding_similarity = wv.similarity(\"excellent\", \"outstanding\")\n",
        "print(\"Similarity between 'excellent' and 'outstanding': \", excellent_outstanding_similarity)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vIiBdocqryr"
      },
      "source": [
        "### Example 3 - Apple ∼ Fruit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4M2sNQoTqrKl",
        "outputId": "a369ba96-e529-4ad5-8119-c57a0ac69455"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 similar words to 'Apple':  [('apple', 1.0), ('apples', 0.7203599214553833), ('pear', 0.6450697183609009), ('fruit', 0.6410146951675415), ('berry', 0.6302295327186584), ('pears', 0.613396167755127), ('strawberry', 0.6058261394500732), ('peach', 0.6025872826576233), ('potato', 0.5960935354232788), ('grape', 0.5935864448547363)]\n"
          ]
        }
      ],
      "source": [
        "vec_Apple=wv['apple']\n",
        "similar_words = wv.similar_by_vector(vec_Apple, topn=10)\n",
        "print(\"Top 10 similar words to 'Apple': \", similar_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWswgrxxqrNu",
        "outputId": "7fa45516-611c-4d80-a56c-1a374c36c83a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity between 'apple' and 'fruit':  0.6410147\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Calculate and print the semantic similarity of \"apple\" and \"fruit\"\n",
        "apple_fruit_similarity = wv.similarity(\"apple\", \"fruit\")\n",
        "print(\"Similarity between 'apple' and 'fruit': \", apple_fruit_similarity)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHWBtl8v2D5V"
      },
      "source": [
        "## b) Training Word2Vec model using our own dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ooXM6_b2D5V",
        "outputId": "f0336e50-a545-4b18-cb63-ce6390a14bfd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dee-t6AusBeD"
      },
      "source": [
        "### creating training data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sEF89qZ2D5V"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Tokenize each review in the reviews_body column\n",
        "review_tokens = review_data[\"review_body\"].apply(word_tokenize)\n",
        "\n",
        "# Convert the list of lists to a list of strings\n",
        "review_strings = [\" \".join(tokens) for tokens in review_tokens]\n",
        "\n",
        "# Convert the list of strings to a list of lists\n",
        "review_lists = [string.split() for string in review_strings]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vD_Djx7jsjXM"
      },
      "source": [
        "### Custom w2v traning model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AL4O__55N4p"
      },
      "outputs": [],
      "source": [
        "# Train a Word2Vec model on the tokenized sentences\n",
        "model = Word2Vec(review_lists, vector_size=300, window=13, min_count=9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtmUBunBspRM"
      },
      "source": [
        "### Example 1 - King - Man + Woman = Queen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-vRw_jJ2D5V"
      },
      "outputs": [],
      "source": [
        "# Calculate and print the semantic similarity of \"king-man+woman\" and \"queen\"\n",
        "\n",
        "king_vec = model.wv[\"King\"]\n",
        "man_vec = model.wv[\"man\"]\n",
        "woman_vec = model.wv[\"woman\"]\n",
        "queen_vec = king_vec - man_vec + woman_vec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Txtt1HY4QK4",
        "outputId": "864cbe8d-10d3-4792-8afa-186f35f1a765"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 similar words to 'king-man+woman':  [('woman', 0.5592942833900452), ('African', 0.5352551341056824), ('caramel', 0.5162470936775208), ('blonde', 0.4962203800678253), ('Asian', 0.4925335645675659), ('American', 0.4676726162433624), ('Cover', 0.4488953948020935), ('tones', 0.4302102327346802), ('brown', 0.42785486578941345), ('gray', 0.42594367265701294)]\n"
          ]
        }
      ],
      "source": [
        "queen_similarities = model.wv.most_similar(queen_vec, topn=10)\n",
        "print(\"Top 10 similar words to 'king-man+woman': \", queen_similarities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vf7kpUrIz3cN",
        "outputId": "7238e1a8-143d-40db-dd4c-390cfb3ed8b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+---------------------+\n",
            "|   Word   |        Score        |\n",
            "+----------+---------------------+\n",
            "|  woman   |  0.5592942833900452 |\n",
            "| African  |  0.5352551341056824 |\n",
            "| caramel  |  0.5162470936775208 |\n",
            "|  blonde  |  0.4962203800678253 |\n",
            "|  Asian   |  0.4925335645675659 |\n",
            "| American |  0.4676726162433624 |\n",
            "|  Cover   |  0.4488953948020935 |\n",
            "|  tones   |  0.4302102327346802 |\n",
            "|  brown   | 0.42785486578941345 |\n",
            "|   gray   | 0.42594367265701294 |\n",
            "+----------+---------------------+\n"
          ]
        }
      ],
      "source": [
        "table = PrettyTable()\n",
        "table.field_names = [\"Word\", \"Score\"]\n",
        "\n",
        "# Add the data to the table\n",
        "for row in queen_similarities:\n",
        "    table.add_row(row)\n",
        "\n",
        "# Print the table\n",
        "print(table)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a-1zxqJtPGT"
      },
      "source": [
        "### Example 2 - Excellent ∼ Outstanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ll0ZEqCl2D5W",
        "outputId": "0a3bb06f-c7c5-427d-813e-823e7af700d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity between 'excellent' and 'outstanding':  0.71553516\n"
          ]
        }
      ],
      "source": [
        "# Calculate and print the semantic similarity of \"excellent\" and \"outstanding\"\n",
        "excellent_outstanding_similarity = model.wv.similarity(\"excellent\", \"outstanding\")\n",
        "print(\"Similarity between 'excellent' and 'outstanding': \", excellent_outstanding_similarity)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39rtropg2D5W",
        "outputId": "9f8fc939-7daa-441f-b7d7-e60e020d58af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 similar words to 'Excellent':  [('excellent', 1.0), ('outstanding', 0.7155351042747498), ('exceptional', 0.7103866338729858), ('awesome', 0.6956540942192078), ('fantastic', 0.6939514875411987), ('incredible', 0.6319559216499329), ('amazing', 0.628711462020874), ('adequate', 0.6269515752792358), ('acceptable', 0.6211880445480347), ('attractive', 0.6057912111282349)]\n"
          ]
        }
      ],
      "source": [
        "vec_excellent=model.wv['excellent']\n",
        "similar_words = model.wv.most_similar(vec_excellent, topn=10)\n",
        "print(\"Top 10 similar words to 'Excellent': \", similar_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oFhmAJO1pyr",
        "outputId": "36ca065d-0df3-4149-804c-42b6d54bea16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 similar words to 'Apple':  [('apple', 0.9999998807907104), ('cider', 0.84869784116745), ('vinegar', 0.726173460483551), ('baking', 0.6765809059143066), ('sea', 0.6538668274879456), ('salt', 0.642741858959198), ('bark', 0.6396810412406921), ('milk', 0.6391778588294983), ('eucalyptus', 0.6306906938552856), ('mixed', 0.6265164613723755)]\n",
            "Similarity between 'apple' and 'fruit':  0.53493583\n"
          ]
        }
      ],
      "source": [
        "vec_Apple=model.wv['apple']\n",
        "similar_words = model.wv.similar_by_vector(vec_Apple, topn=10)\n",
        "print(\"Top 10 similar words to 'Apple': \", similar_words)\n",
        "\n",
        "# Calculate and print the semantic similarity of \"apple\" and \"fruit\"\n",
        "apple_fruit_similarity = model.wv.similarity(\"apple\", \"fruit\")\n",
        "print(\"Similarity between 'apple' and 'fruit': \", apple_fruit_similarity)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###### In the first example (King-Man+Woman=Queen), the \"word2vec-google-news-300\" generated the expected output \"Queen\" as one of the top similar words, while our trained model did not. Additionally, the \"word2vec-google-news-300\" model seemed to generate more semantically similar words overall. For the second example (excellent ~ outstanding), the trained Word2Vec model performed better, with a higher similarity score between the two words compared to the \"word2vec-google-news-300\" model. In the third example (apple ~ fruit), the \"word2vec-google-news-300\" model performed better, with a higher similarity score between the two words compared to the trained Word2Vec model.\t\n",
        "\n",
        "###### However, it's challenging to conclude which Word2Vec model is better at encoding semantic similarities between words based solely on these examples. Each model performed better for different examples, and the quality of the similarity scores can depend on various factors, including the training data's size and quality, the vector space's dimensionality, and the training model's specific parameters.\n",
        "\n",
        "###### In general, Word2Vec models are effective at capturing semantic similarities between words, but the performance can vary depending on the training data and parameters. Pretrained Word2Vec models like \"word2vec-google-news-300\" are often trained on large amounts of high-quality text data and are better at capturing a wide range of semantic similarities between words. On the other hand, Word2Vec models trained on specific domains or datasets can capture domain-specific semantic relationships more effectively but may not generalize well to other domains. Therefore, the performance of a Word2Vec model in capturing semantic similarities between words will depend on the specific use case, training data, and parameters used to train the model."
      ],
      "metadata": {
        "id": "8O0PyqlCkYhv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip3-hmo_2D5W"
      },
      "source": [
        "# 3. Simple models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "KL_Lg8612D5W"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMx-sd3Su4rX"
      },
      "source": [
        "### Averaging Word2Vec vectors for each review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "lbm44q062D5X"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create input features and output labels for training data\n",
        "X_train_w2v = np.zeros((len(train_data), 300)) # each row represents a review with 300 dimensions\n",
        "y_train = train_data['star_rating'].values\n",
        "\n",
        "# Compute average Word2Vec vectors for each review in training data\n",
        "for i, review in enumerate(train_data['review_body']):\n",
        "    words = review.split()\n",
        "    vectors = [wv[word] for word in words if word in wv]\n",
        "    if vectors:\n",
        "        X_train_w2v[i] = np.mean(vectors, axis=0)\n",
        "\n",
        "# Create input features and output labels for testing data\n",
        "X_test_w2v = np.zeros((len(test_data), 300)) # each row represents a review with 300 dimensions\n",
        "y_test = test_data['star_rating'].values\n",
        "\n",
        "# Compute average Word2Vec vectors for each review in testing data\n",
        "for i, review in enumerate(test_data['review_body']):\n",
        "    words = review.split()\n",
        "    vectors = [wv[word] for word in words if word in wv]\n",
        "    if vectors:\n",
        "        X_test_w2v[i] = np.mean(vectors, axis=0)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hboRg8vmvSDG"
      },
      "source": [
        "## Single Perceptron "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9NjMBBY2D5X",
        "outputId": "3c13163d-56ef-449e-8cdc-8429bea82aa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perceptron Accuracy: 0.537\n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate perceptron model\n",
        "perceptron = Perceptron()\n",
        "perceptron.fit(X_train_w2v, y_train)\n",
        "y_pred = perceptron.predict(X_test_w2v)\n",
        "print(\"Perceptron Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7oRf1dxvY_o"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9v46v40I2D5X",
        "outputId": "9f0f224d-42a3-4437-9e73-fbc4eb00f564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy: 0.6350833333333333\n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate SVM model\n",
        "svm = LinearSVC()\n",
        "svm.fit(X_train_w2v, y_train)\n",
        "y_pred = svm.predict(X_test_w2v)\n",
        "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### After comparing the performance of the models trained using TF-IDF and trained Word2Vec features, we can conclude that the models trained with TF-IDF features performed better overall. This suggests that TF-IDF features are more effective in capturing the necessary information for this specific classification task.\n",
        "\n",
        "###### However, it's important to note that the difference in accuracy between the two feature types is not significant, indicating that both feature types have the potential to be effective to some extent. While trained Word2Vec features did not perform as well in this specific task, they may be more effective in other classification tasks or domains. "
      ],
      "metadata": {
        "id": "ZbP4yJEqkwWU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6SM621i2D5Y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNMH3iWL2D5Y"
      },
      "source": [
        "\n",
        "\n",
        "# 4. Feedforward Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08veIeKUxVow"
      },
      "source": [
        "## a) the average Word2Vec vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2IsbCwQwr6d"
      },
      "source": [
        "### loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "NP3qRZB_--Ut"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.X = np.zeros((len(data), 300)) # each row represents a review with 300 dimensions\n",
        "        self.y = data['star_rating'].values - 1 # convert to 0-indexed labels\n",
        "        for i, review in enumerate(data['review_body']):\n",
        "            words = review.split()\n",
        "            vectors = [wv[word] for word in words if word in wv]\n",
        "            if vectors:\n",
        "                self.X[i] = np.mean(vectors, axis=0)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return torch.from_numpy(self.X[idx]), torch.tensor(self.y[idx])\n",
        "\n",
        "# Create datasets and data loaders for training and testing\n",
        "train_dataset = ReviewDataset(train_data)\n",
        "test_dataset = ReviewDataset(test_data)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Svo-skowu9y"
      },
      "source": [
        "### FNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Ptnn84snl2ib"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(300, 100)\n",
        "        self.dropout1 = nn.Dropout(0)\n",
        "        self.fc2 = nn.Linear(100, 10)\n",
        "        self.dropout2 = nn.Dropout(0)\n",
        "        self.fc3 = nn.Linear(10, 3)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.dropout1(torch.relu(self.fc1(x)))\n",
        "        x = self.dropout2(torch.relu(self.fc2(x)))\n",
        "        x = nn.functional.softmax(self.fc3(x), dim=1)\n",
        "        return x\n",
        "\n",
        "# Instantiate the network and the optimizer\n",
        "net = Net()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.005)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOaDSjD2w1tk"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "GG5b-EgT7HX9",
        "outputId": "5b97af1c-7032-429e-ac73-78fca9f3030a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.9442, Accuracy: 57.95%\n",
            "Epoch 2, Loss: 0.9100, Accuracy: 62.39%\n",
            "Epoch 3, Loss: 0.9036, Accuracy: 63.06%\n",
            "Epoch 4, Loss: 0.8967, Accuracy: 63.91%\n",
            "Epoch 5, Loss: 0.8948, Accuracy: 64.36%\n",
            "Epoch 6, Loss: 0.8911, Accuracy: 64.68%\n",
            "Epoch 7, Loss: 0.8881, Accuracy: 65.05%\n",
            "Epoch 8, Loss: 0.8878, Accuracy: 64.95%\n",
            "Epoch 9, Loss: 0.8838, Accuracy: 65.45%\n",
            "Epoch 10, Loss: 0.8812, Accuracy: 65.69%\n",
            "Epoch 11, Loss: 0.8772, Accuracy: 66.22%\n",
            "Epoch 12, Loss: 0.8767, Accuracy: 66.26%\n",
            "Epoch 13, Loss: 0.8734, Accuracy: 66.61%\n",
            "Epoch 14, Loss: 0.8712, Accuracy: 66.86%\n",
            "Epoch 15, Loss: 0.8680, Accuracy: 67.24%\n",
            "Epoch 16, Loss: 0.8662, Accuracy: 67.45%\n",
            "Epoch 17, Loss: 0.8634, Accuracy: 67.71%\n",
            "Epoch 18, Loss: 0.8633, Accuracy: 67.83%\n",
            "Epoch 19, Loss: 0.8608, Accuracy: 68.10%\n",
            "Epoch 20, Loss: 0.8583, Accuracy: 68.40%\n",
            "Epoch 21, Loss: 0.8563, Accuracy: 68.55%\n",
            "Epoch 22, Loss: 0.8539, Accuracy: 68.87%\n",
            "Epoch 23, Loss: 0.8520, Accuracy: 69.12%\n",
            "Epoch 24, Loss: 0.8515, Accuracy: 69.18%\n",
            "Epoch 25, Loss: 0.8481, Accuracy: 69.51%\n",
            "Testing accuracy: 63.90%\n"
          ]
        }
      ],
      "source": [
        "# Train the network\n",
        "for epoch in range(25):\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for X, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = net(X.float())\n",
        "        loss = nn.functional.cross_entropy(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Calculate running loss and accuracy\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total += y.size(0)\n",
        "        correct += (predicted == y).sum().item()\n",
        "\n",
        "    # Print epoch loss and accuracy\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    print(f'Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n",
        "    \n",
        "# Evaluate the network on the test set\n",
        "y_pred = []\n",
        "y_true = []\n",
        "with torch.no_grad():\n",
        "    for X, y in test_loader:\n",
        "        output = net(X.float())\n",
        "        _, pred = torch.max(output, 1)\n",
        "        y_pred.extend(pred.numpy())\n",
        "        y_true.extend(y.numpy())\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f'Testing accuracy: {accuracy*100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBEndeDS2D5Y"
      },
      "source": [
        "## b) concatenate the first 10 Word2Vec vectors for each review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWZINYUPxrDI"
      },
      "source": [
        "### data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WEBWN07NO642"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.X = np.zeros((len(data), 3000))\n",
        "        self.y = data['star_rating'].values - 1 # convert to 0-indexed labels\n",
        "        for i, review in enumerate(data['review_body']):\n",
        "            words = review.split()\n",
        "            vectors = [wv[word] for word in words if word in wv ][:10]\n",
        "            if len(vectors) < 10:\n",
        "                vectors += [np.zeros(300)] * (10 - len(vectors))\n",
        "            self.X[i] = np.concatenate(vectors)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return torch.from_numpy(self.X[idx]), torch.tensor(self.y[idx])\n",
        "\n",
        "# Create datasets and data loaders for training and testing\n",
        "train_dataset = ReviewDataset(train_data)\n",
        "test_dataset = ReviewDataset(test_data)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SPU2mtOxx4c"
      },
      "source": [
        "### MLP Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0pcKkuejxxDw"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the dimensions of the input and output layers\n",
        "input_dim = 3000\n",
        "output_dim = 3\n",
        "\n",
        "hidden_dim1 = 100\n",
        "hidden_dim2 = 10\n",
        "\n",
        "dropout_rate1 = dropout_rate2 = 0\n",
        "\n",
        "# Define the  architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate1)\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate2)\n",
        "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.dropout1(torch.relu(self.fc1(x)))\n",
        "        x = self.dropout2(torch.relu(self.fc2(x)))\n",
        "        x = nn.functional.softmax(self.fc3(x), dim=1)\n",
        "        return x\n",
        "\n",
        "# Instantiate the network and the optimizer\n",
        "net = Net()\n",
        "learning_rate = 0.001\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxBMf-PFyEyg"
      },
      "source": [
        "### training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9FRb5KJZ7XhR",
        "outputId": "f5c02639-7ab7-47af-8547-fb0777ccd62e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.9868, Accuracy: 53.61%\n",
            "Epoch 2, Loss: 0.9391, Accuracy: 59.25%\n",
            "Epoch 3, Loss: 0.9102, Accuracy: 62.73%\n",
            "Epoch 4, Loss: 0.8759, Accuracy: 66.76%\n",
            "Epoch 5, Loss: 0.8404, Accuracy: 70.67%\n",
            "Epoch 6, Loss: 0.8079, Accuracy: 74.20%\n",
            "Epoch 7, Loss: 0.7844, Accuracy: 76.66%\n",
            "Epoch 8, Loss: 0.7659, Accuracy: 78.49%\n",
            "Epoch 9, Loss: 0.7539, Accuracy: 79.72%\n",
            "Epoch 10, Loss: 0.7444, Accuracy: 80.60%\n",
            "Epoch 11, Loss: 0.7387, Accuracy: 81.22%\n",
            "Epoch 12, Loss: 0.7309, Accuracy: 82.08%\n",
            "Epoch 13, Loss: 0.7278, Accuracy: 82.31%\n",
            "Epoch 14, Loss: 0.7250, Accuracy: 82.60%\n",
            "Epoch 15, Loss: 0.7210, Accuracy: 82.99%\n",
            "Epoch 16, Loss: 0.7189, Accuracy: 83.17%\n",
            "Epoch 17, Loss: 0.7156, Accuracy: 83.50%\n",
            "Epoch 18, Loss: 0.7140, Accuracy: 83.63%\n",
            "Epoch 19, Loss: 0.7106, Accuracy: 84.03%\n",
            "Epoch 20, Loss: 0.7098, Accuracy: 84.09%\n",
            "Epoch 21, Loss: 0.7072, Accuracy: 84.34%\n",
            "Epoch 22, Loss: 0.7060, Accuracy: 84.45%\n",
            "Epoch 23, Loss: 0.7041, Accuracy: 84.64%\n",
            "Epoch 24, Loss: 0.7020, Accuracy: 84.88%\n",
            "Epoch 25, Loss: 0.7018, Accuracy: 84.87%\n",
            "Epoch 26, Loss: 0.6982, Accuracy: 85.25%\n",
            "Epoch 27, Loss: 0.6976, Accuracy: 85.34%\n",
            "Epoch 28, Loss: 0.6987, Accuracy: 85.16%\n",
            "Epoch 29, Loss: 0.6971, Accuracy: 85.35%\n",
            "Epoch 30, Loss: 0.6962, Accuracy: 85.42%\n",
            "Epoch 31, Loss: 0.6942, Accuracy: 85.68%\n",
            "Epoch 32, Loss: 0.6953, Accuracy: 85.55%\n",
            "Epoch 33, Loss: 0.6932, Accuracy: 85.75%\n",
            "Epoch 34, Loss: 0.6936, Accuracy: 85.72%\n",
            "Epoch 35, Loss: 0.6922, Accuracy: 85.84%\n",
            "Epoch 36, Loss: 0.6914, Accuracy: 85.91%\n",
            "Epoch 37, Loss: 0.6901, Accuracy: 86.05%\n",
            "Epoch 38, Loss: 0.6900, Accuracy: 86.06%\n",
            "Epoch 39, Loss: 0.6887, Accuracy: 86.17%\n",
            "Epoch 40, Loss: 0.6885, Accuracy: 86.25%\n",
            "Epoch 41, Loss: 0.6875, Accuracy: 86.32%\n",
            "Epoch 42, Loss: 0.6871, Accuracy: 86.37%\n",
            "Epoch 43, Loss: 0.6857, Accuracy: 86.50%\n",
            "Epoch 44, Loss: 0.6839, Accuracy: 86.69%\n",
            "Epoch 45, Loss: 0.6844, Accuracy: 86.64%\n",
            "Epoch 46, Loss: 0.6853, Accuracy: 86.55%\n",
            "Epoch 47, Loss: 0.6846, Accuracy: 86.59%\n",
            "Epoch 48, Loss: 0.6823, Accuracy: 86.82%\n",
            "Epoch 49, Loss: 0.6830, Accuracy: 86.76%\n",
            "Epoch 50, Loss: 0.6817, Accuracy: 86.91%\n",
            "Testing accuracy: 55.57%\n"
          ]
        }
      ],
      "source": [
        "# Train the network\n",
        "for epoch in range(50):\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for X, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = net(X.float())\n",
        "        loss = nn.functional.cross_entropy(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Calculate running loss and accuracy\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total += y.size(0)\n",
        "        correct += (predicted == y).sum().item()\n",
        "\n",
        "    # Print epoch loss and accuracy\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    print(f'Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n",
        "    \n",
        "# Evaluate the network on the test set\n",
        "y_pred = []\n",
        "y_true = []\n",
        "with torch.no_grad():\n",
        "    for X, y in test_loader:\n",
        "        output = net(X.float())\n",
        "        _, pred = torch.max(output, 1)\n",
        "        y_pred.extend(pred.numpy())\n",
        "        y_true.extend(y.numpy())\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f'Testing accuracy: {accuracy*100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###### Based on the comparison of accuracy values, we can conclude that the feedforward neural network performed better than the simple models. The average Word2Vec model achieved an accuracy of 63.90%, which is higher than the accuracy values obtained from the Perceptron and SVM models. However, the concatenate model with the first 10 Word2Vec models had a lower accuracy of only 55.57%, which is worse than the simple models.\n",
        "\n",
        "###### In summary, the performance of the feedforward neural network was mixed compared to the simple models, with one model performing significantly better and one model performing worse. This suggests that the effectiveness of different models can vary depending on the specific features and parameters used."
      ],
      "metadata": {
        "id": "WXqMLz7Dl8Ke"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyDtJtdD2D5Z"
      },
      "source": [
        "# 5. Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW3VdpwWyzph"
      },
      "source": [
        "## RNN Cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "-e1th6aei56N"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim.downloader as api\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "eqaesTD2ncpm"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, data, max_len=20):\n",
        "        self.data = data\n",
        "        self.X = np.zeros((len(data), max_len, 300)) # each row represents a review with 300 dimensions\n",
        "        self.y = data['star_rating'].values - 1 # convert to 0-indexed labels\n",
        "        self.max_len = max_len\n",
        "        for i, review in enumerate(data['review_body']):\n",
        "            words = review.split()\n",
        "            words = [word for word in words if word in wv ][:max_len]\n",
        "            for j, word in enumerate(words):\n",
        "              self.X[i][j] = wv[word]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return torch.from_numpy(self.X[idx]), torch.tensor(self.y[idx])\n",
        "\n",
        "\n",
        "\n",
        "# Create datasets and data loaders for training and testing\n",
        "train_dataset = ReviewDataset(train_data)\n",
        "test_dataset = ReviewDataset(test_data)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        h0 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
        "        out, hidden = self.rnn(x, h0)\n",
        "        out = self.fc(hidden[-1])\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-qKX_hAi2w4",
        "outputId": "0b430e4d-6f68-4ff1-baf4-b8c5e8a599d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.0175, Accuracy: 46.37%\n",
            "Epoch 2, Loss: 0.9432, Accuracy: 53.62%\n",
            "Epoch 3, Loss: 0.9148, Accuracy: 55.78%\n",
            "Epoch 4, Loss: 0.8976, Accuracy: 57.31%\n",
            "Epoch 5, Loss: 0.8853, Accuracy: 58.22%\n",
            "Epoch 6, Loss: 0.8772, Accuracy: 58.84%\n",
            "Epoch 7, Loss: 0.8702, Accuracy: 59.15%\n",
            "Epoch 8, Loss: 0.8655, Accuracy: 59.41%\n",
            "Epoch 9, Loss: 0.8595, Accuracy: 59.84%\n",
            "Epoch 10, Loss: 0.8531, Accuracy: 60.26%\n",
            "Testing accuracy: 59.48%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Instantiate the network and the optimizer\n",
        "net = RNN(input_size=300, hidden_size=20, output_size=3)\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# Train the network\n",
        "for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    \n",
        "    for X, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = net(X.float())\n",
        "        loss = nn.functional.cross_entropy(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        _, pred = torch.max(output, 1)\n",
        "        correct_predictions += (pred == y).sum().item()\n",
        "        total_predictions += len(y)\n",
        "    \n",
        "    # Compute the accuracy and loss for this epoch\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = 100 * correct_predictions / total_predictions\n",
        "    \n",
        "    # Print the epoch number, accuracy and loss\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "# Evaluate the network on the test set\n",
        "y_pred = []\n",
        "y_true = []\n",
        "with torch.no_grad():\n",
        "    for X, y in test_loader:\n",
        "        output = net(X.float())\n",
        "        _, pred = torch.max(output, 1)\n",
        "        y_pred.extend(pred.numpy())\n",
        "        y_true.extend(y.numpy())\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f'Testing accuracy: {accuracy*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L94J72Vmy_JN"
      },
      "source": [
        "## b) Gated Recurrent Unit Cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZq8w7zri21q",
        "outputId": "d33075df-6aab-4b23-fbc2-552a2b7a2264"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.9353, Accuracy: 52.90%\n",
            "Epoch 2, Loss: 0.8154, Accuracy: 62.26%\n",
            "Epoch 3, Loss: 0.7836, Accuracy: 64.06%\n",
            "Epoch 4, Loss: 0.7650, Accuracy: 65.20%\n",
            "Epoch 5, Loss: 0.7488, Accuracy: 66.02%\n",
            "Epoch 6, Loss: 0.7354, Accuracy: 66.84%\n",
            "Epoch 7, Loss: 0.7247, Accuracy: 67.44%\n",
            "Epoch 8, Loss: 0.7132, Accuracy: 68.13%\n",
            "Epoch 9, Loss: 0.7031, Accuracy: 68.59%\n",
            "Epoch 10, Loss: 0.6949, Accuracy: 69.24%\n",
            "Testing accuracy: 64.42%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim.downloader as api\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from typing import List\n",
        "\n",
        "\n",
        "\n",
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(GRU, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        h0 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
        "        out, hidden = self.gru(x, h0)\n",
        "        out = self.fc(hidden[-1])\n",
        "        return out\n",
        "\n",
        "\n",
        "# Instantiate the network and the optimizer\n",
        "net = GRU(input_size=300, hidden_size=20, output_size=3)\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# Train the network\n",
        "for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    \n",
        "    for X, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = net(X.float())\n",
        "        loss = nn.functional.cross_entropy(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        _, pred = torch.max(output, 1)\n",
        "        correct_predictions += (pred == y).sum().item()\n",
        "        total_predictions += len(y)\n",
        "    \n",
        "    # Compute the accuracy and loss for this epoch\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = 100 * correct_predictions / total_predictions\n",
        "    \n",
        "    # Print the epoch number, accuracy and loss\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "# Evaluate the network on the test set\n",
        "y_pred = []\n",
        "y_true = []\n",
        "with torch.no_grad():\n",
        "    for X, y in test_loader:\n",
        "        output = net(X.float())\n",
        "        _, pred = torch.max(output, 1)\n",
        "        y_pred.extend(pred.numpy())\n",
        "        y_true.extend(y.numpy())\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f'Testing accuracy: {accuracy*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo3vbVqYzJep"
      },
      "source": [
        "## LSTM Cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "kSSwCW-vi27H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19c78626-0306-4491-af5d-9ddbf9e2d024"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.9502, Accuracy: 51.50%\n",
            "Epoch 2, Loss: 0.8431, Accuracy: 60.82%\n",
            "Epoch 3, Loss: 0.8030, Accuracy: 63.15%\n",
            "Epoch 4, Loss: 0.7800, Accuracy: 64.47%\n",
            "Epoch 5, Loss: 0.7617, Accuracy: 65.37%\n",
            "Epoch 6, Loss: 0.7450, Accuracy: 66.38%\n",
            "Epoch 7, Loss: 0.7343, Accuracy: 66.84%\n",
            "Epoch 8, Loss: 0.7182, Accuracy: 67.89%\n",
            "Epoch 9, Loss: 0.7071, Accuracy: 68.48%\n",
            "Epoch 10, Loss: 0.6966, Accuracy: 69.10%\n",
            "Testing accuracy: 65.07%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        h0 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
        "        out, (hidden, cell) = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(hidden[-1])\n",
        "        return out\n",
        "\n",
        "\n",
        "# Instantiate the network and the optimizer\n",
        "net = LSTM(input_size=300, hidden_size=20, output_size=3)\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# Train the network\n",
        "for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    \n",
        "    for X, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = net(X.float())\n",
        "        loss = nn.functional.cross_entropy(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        _, pred = torch.max(output, 1)\n",
        "        correct_predictions += (pred == y).sum().item()\n",
        "        total_predictions += len(y)\n",
        "    \n",
        "    # Compute the accuracy and loss for this epoch\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = 100 * correct_predictions / total_predictions\n",
        "    \n",
        "    # Print the epoch number, accuracy and loss\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "# Evaluate the network on the test set\n",
        "y_pred = []\n",
        "y_true = []\n",
        "with torch.no_grad():\n",
        "    for X, y in test_loader:\n",
        "        output = net(X.float())\n",
        "        _, pred = torch.max(output, 1)\n",
        "        y_pred.extend(pred.numpy())\n",
        "        y_true.extend(y.numpy())\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f'Testing accuracy: {accuracy*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Comparing the accuracy values obtained with the RNN cell and the feedforward neural network models, we can conclude that the feedforward neural network models performed slightly better. The average Word2Vec feedforward neural network model achieved an accuracy of 63.90%, which is higher than the accuracy of the RNN cell model at 59.48%. However, the concatenate model with the first 10 Word2Vec models did not perform as well, with an accuracy of only 55.57%. \n",
        "\n",
        "###### By comparing the accuracy values obtained with the GRU, LSTM, and simple RNN models, we can conclude that the more complex models, GRU and LSTM, outperformed the simple RNN model. The LSTM model achieved the highest accuracy of 65.07%, followed by the GRU model at 64.42%, while the simple RNN model achieved an accuracy of 59.48%. These results suggest that the added complexity of the GRU and LSTM models, with their ability to better handle long-term dependencies, improved their ability to classify the reviews correctly."
      ],
      "metadata": {
        "id": "w6Xz01bgmEwe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yf0uTEni3BF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "c48128d67144545eafac6f03b912da95004fab4577e10cc38403cc965373c836"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}